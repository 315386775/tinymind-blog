---
title: 深度学习实战技巧
date: 2024-09-27T05:57:11.764Z
---


## 训练
LP-FT(linear-probing and then full fine-tuning,ICLR 2022 0ral)，预训练模型微调+和OODgeneralization领域一个非常简洁有效的idea: 为了避免微调过度破坏预训练表征、影响模型泛化能力，先固定模型主干，训练分类头(也就是linear-probing)，让分类头有一个比较好的初始化之后，再打开所有参数一起训练(也就是full fine-tuning)，可以显著提升微调后模型的OODgeneralization (分布外数据上的泛化性能+)。

"How to prepare your task head for finetuning." ICLR 2023